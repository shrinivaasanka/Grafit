##############################################################################################################################################
<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.
##############################################################################################################################################
Course Authored By:
-----------------------------------------------------------------------------------------------------------
Srinivasan Kannan
(also known as: Shrinivaasan Kannan, Shrinivas Kannan)
Ph: 9791499106, 9003082186
Krishna iResearch Open Source Products Profiles:
http://sourceforge.net/users/ka_shrinivaasan,
https://github.com/shrinivaasanka,
https://www.openhub.net/accounts/ka_shrinivaasan
Personal website(research): https://sites.google.com/site/kuja27/
emails: ka.shrinivaasan@gmail.com, shrinivas.kannan@gmail.com,
kashrinivaasan@live.com
-----------------------------------------------------------------------------------------------------------
##############################################################################################################################################

This is a non-linearly organized, code puzzles oriented, continually updated set of course notes on Cloud computing frameworks and 
BigData analysis. 
----------------------------------------------------------------------------------------------------------------------------

7 Februrary 2017
----------------
Apache Spark is a Cloud computing software for processing bigdata. It is based on concept of Resilient Distributed Datasets (RDD) which are partitions of a dataset executed in parallel. Spark is divided into 2 components: 1) Driver and 2) Executor. Driver splits the dataset into RDDs and allocates each RDD to an executor in parallel. Parallelization can be in two ways: 1) For objects like lists,arrays etc., 2) For data in HDFS,cassandra, S3 etc.,  

While executing in parallel, there is a necessity to share mutable state across executors. This is done in two ways: Broadcast variables and Accumulators (only increment is allowed). Spark streaming is a feature that allows realtime processing of streaming data by an abstraction of Discretized Streams or DStreams. Following code in neuronrain asfer receives generic data from any URL, does an ETL on it and stores in RDDs:
https://github.com/shrinivaasanka/asfer-github-code/blob/master/java-src/bigdata_analytics/spark_streaming/SparkGenericStreaming.java

There are 2 operations performed on RDDs: 1) Transformations - create a new set or subset of RDDs 2) Actions - do some iteration on transformed RDDs. Spark streaming allows custom streaming by implementing/overriding receive() method in Receiver interface. Receiver can be started and stopped by onStart() and onStop() methods. Receive method is overridden with customized code to access a remote URL, fetch HTML, parse it and do any ETL operation as deemed fit. From Spark 2.0.0 , support for lambda functions (new feature in Java 8) instead of *.function.* (Function objects) for RDD transformations has been included. Previous Spark Streaming code demonstrates these features and uses Jsoup GET RESTful API for ETL/scraping of remote URL data.

20 February 2017
----------------
Spark SQL + Hive (Shark) provides synergy of bigdata processing with an SQL storage backend. Hive is implemented on top of Thrift RPC protocol  which is modern version of Interface Definition Language based Web Service Architectures like CORBA, Google Protocol buffers , SOAP etc., Streamed data received is an iterable (e.g lines in SparkGenericStreaming.java implementation in https://github.com/shrinivaasanka/asfer-github-code/blob/master/java-src/bigdata_analytics/spark_streaming/SparkGenericStreaming.java) which is further transformed with map/filter operations quite similar to Java Streams. Java Streams work on similar concept of creating a stream from iterable (arrays, lists etc.,) and applying map/filter transformations. Spark's saveAsTable() saves the streaming data into a hive table in Spark Metastore or Hive metastore (this requires hive-site.xml in Spark conf directory). (MAC currency in AsFer+KingCobra electronic money cloud perfect forwarding move is implemented on Protocol Buffers.)

18 January 2018
---------------
Spark cloud processing framework has support for global variables in two flavours: 1) Accumulators and 2) Broadcast variables. Both of these are mechanisms to reflect global state across Resilient Distributed Data Set nodes in Spark clusters. Accumulators have a single operation add() which incrementally adds a value on a local RDD to the global accumulator variable and is reflected across all nodes in Spark cluster. Both Accumulators and Broadcast variables are instantiated from Spark Context. Broadcast variables are plain read-only global variables which are broadcast as the name suggests to all RDDs in Spark cluster. An example code and logs for how accumulators and broadcast work has been demonstrated in code/Spark_Broadcast_Accumulator.py and code/testlogs/Spark_Broadcast_Accumulator.log.18January2018. Accumulator constructor can be optionally passed on an object of type AccumulatorParam (or its subclassed types which override add()). Presently accumulators and broadcasts are only way to provide global state across nodes in Spark cluster.

----------------------------------------------------------------------------------------------------------
4 October 2018 - Representational State Transfer - CRUD - RESTful and WebServices in Cloud
----------------------------------------------------------------------------------------------------------
Traditional Client-Server Architecture in Distributed Computing involves client making a socket connection to
a listener server, completing a handshake and establishing a two-way message transport.Over the years, with the
advent of cloud, every application on web is deemed to be a finite state automaton of 4 states - Create-Read-Update-Delete (CRUD) respective HTTP primitives being PUT,GET,POST,DELETE which create a resource in server, update it, read it and delete. Every resource is identified by a URL or WebService. Though this indirectly wraps the underlying socket communication, benefit is in statelessness of each request - every request is independent of previous request and state is remembered only in client side and server is state oblivious. Nomenclature RESTful stems from the state getting transferred from client to server for every HTTP request and responded in JSON objects. An example of RESTful API is Facebook Graph API SDK for retrieving user profile information, connections, comments, likes etc., A REST Python Client which GETs/PUTs objects to Facebook wall has been described in code/GRAFIT_automatic_wallposter.py. It internally issues HTTP requests to Graph API REST endpoints. Invocation to put_object() has been commented. This is an updated version of https://github.com/shrinivaasanka/asfer-github-code/blob/master/python-src/Streaming_FacebookData.py specific to GRAFIT (for Grafit Open Learning facebook profile https://www.facebook.com/shrinivaasan.ka which imports @NeuronRain_Comm - https://twitter.com/neuronrain_comm - automatic commit tweets). RESTful implies every cloud distributed computation is a string from alphabets {GET, POST, PUT, DELETE} amongst the nodes in cloud URL graph.

-------------------------------------------------------------------------------------------------------------
5 December 2018 - Apache 2 Web Server, Apache HTTPD modules, Configs and Hooks, Application Servers - example
------------------------------------------------------------------------------------------------------------- 
Apache webserver provides facilities to plugin user developed modules. One specific standard example is mod_ssl which is an SSL plugin module for Apache webserver. An example Apache module implementation and related forum Q&A have been cited in the references. This implementation was a part of Sun Microsystems/Oracle iPlanet Application Server which had a 3-tier J2EE compliant middleware architecture (Clients <-> WebServer <-> ApplicationServer). Application Servers are ancestors of present day cloud implementations which are Service Oriented Architectures. HTTP requests are served by Apache Webserver and responded. Necessity for an Apache module arises when Apache webserver is used as a loadbalancer for a cluster of application servers and requests have to be routed to it. Example module snippet in the reference defines config parameters for Apache webserver - the loadbalancer XML file containing details about cluster of iPlanet Application Servers and Locale. The register_hooks() function has callback functions for init, name translation (e.g. URL rewriting for session ids and cookies), and handle requests (e.g routing requests to another application server). These config parameters are set by invoking ap_set_string_slot() functions denoted by assignment to take1 structure member (function takes 1 argument)

References:
-----------
1.Apache Modules Mailing list - Apache 1.3.27 and iPlanet Application Server - https://marc.info/?l=apache-modules&m=105610024116012 (Copyright: Sun Microsystems/Oracle)
2.Apache Modules Mailing list - Apache 2.0.47 and iPlanet Application Server - https://marc.info/?l=apache-modules&m=106267009905554 (Copyright: Sun Microsystems/Oracle)
3.Apache Config Directives - ApacheCon - http://events17.linuxfoundation.org/sites/events/files/slides/ConfigurationDirectiveAPI.pdf

------------------------------------------------------------------------------------------------------------
2 January 2019, 9 January 2019 - Searching, Indexing - Binary Search Nuances, B-Trees, Sharding
------------------------------------------------------------------------------------------------------------
Traditional Binary Search still remains the standard for sifting huge datasets and has undergone significant
refinements. Usual Binary Search is centred around midpoint computation and branching:
	midpoint = (left + right)/2
	if query > midpoint:
		search interval [midpoint, right] 
	else:
		search interval [left, midpoint]

Bug in Midpoint computation by previous averaging in some earlier versions of languages like Java caused an overflow error in 32-bit architectures which necessitated replacing it by >>> operator (unsigned right shift):
	midpoint = (right-left) >>> 1;
B-Trees are generalized Binary Search Trees in which each internal and root nodes contain more than one element and every element of a node acts as a separator for values of its subtrees as in example:
				2,10
				|
			1 ----- 3,9 ---- 11,12
				|
				4,5
B+-Trees extend B-Trees by additional linked-list of leaf nodes.

Indexing Bigdata e.g web pages involves storing them as huge set of key-value pairs of the form:
		word	--- (document1, location1), (document2, location2) ...
which is termed as posting in inverted indices. Size of an Index in search engine could be unimaginably large requiring partitions into set of rows located across geographically distant servers on cloud. This is horizontal partition based on rows as against vertical partitions of columns in DBMS Normalizations. Each partition is named a Shard.

Fundamental question is: can items in a list be found efficiently by a search graph instead of binary search trees. Lower bound for search is Omega(logN). Binary Search Trees can be alternatively defined as recursive bipartite graph which is an indefinite recursive fractal bipartition of sets - Example:- Set of integers [2,4,5,1,3,7,9,10,8,6] are represented recursively as bipartite graph by assuming an initial midpoint pivot separator of each subset similar to quicksort which partitions a set into two halves of elements > pivot and elements < pivot in each depth of recursion:
	pivot 6
	[2,3,5,1,4] ----- [7,9,10,8,6]
	pivot 3			pivot 8
	[[4,5,3]--[2,1]] ---- [[6,8,7]--[10,9]]
	pivot 4			pivot 7
	[[[4,3]-[5]]--[2,1]] ---- [[[7,8]-[6]]---[[10]-[9]]]
	pivot 4		pivot 2			pivot 7
	[[[[4]-[3]]-[5]]--[[2]-[1]]] ---- [[[[7]-[8]]-[6]]---[[10]-[9]]]

Edges are denoted by ----- which connect each parenthesized subset vertex for a subtree obtained by traversing the binary search tree. Last line encodes a recursive bipartite search graph which is the top view of the binary search tree and is a multidimensional tensor - a kind of binary space partition. Searching this tensor locates a point in the nested parentheses by comparing against pivots. Advantage of recursive bipartite graph representation of binary search tree is each subtree can be retrieved by array indexing.
		
References:
-----------
1.The Art of Computer Programming - Volume 3 - [Donald Knuth] - Sorting and Searching - 6.2.2
2.Beautiful Code - Finding Things - [Tim Bray - Sun Microsystems]
3.Google AI Blog - [Joshua Bloch] - https://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html
4.Topological Sorting - Fast Parallel Algorithms - [SA Cook] - https://www.sciencedirect.com/science/article/pii/S0019995885800413 - If set of integers are randomly assigned as labels of a random graph, topological sorting of vertices by some ordering can be done efficiently in NC^2 and a following binary search can find the element in additional logarithmic time. But the topologically sorted vertex list may not be sorted by total ordering always.Dekel-Nassimi-Sahni algorithm works by Repeated Matrix Multiplication logarithmically many times and sorting by longest paths.

-----------------------------------------------------------------------------------------------------
21 January 2019 - FP Growth Algorithm for mining frequent patterns and mining timeout dictionaries
example
-----------------------------------------------------------------------------------------------------
Timeout design pattern implemented as a separate chaining/dictionary of timeouts-to-processes has been described in ../AdvancedComputerScienceAndMachineLearning/AdvancedComputerScienceAndMachineLearning.txt. Discussion there is restricted to a process id present in only one bucket per timeout value. But there are possibilities when same process_id has to be multilocated in many timeout buckets e.g multiple threads spawned by a process can have different timer values for varied functionalities and all those threads per process have to be timedout. 

Spark MLlib implements parallel version of FP-Growth algorithm  which mines frequent itemsets in multiple baskets with no candidate generation (downward closure) by growing suffix trees. Survival Index Timeout Map is also a set of baskets (buckets) from which frequently occurring process subsets can be mined by Spark FPGrowth. Example Spark code for this has been committed to code/Spark_FPGrowth_SurvivalIndexTimeout.py and logs in code/testlogs/Spark_FPGrowth_SurvivalIndexTimeout.log.21January2019 which demonstrate minimum support for frequent occurrences and minimum confidence for association rules. These frequently occurring process subsets point to some lurking relationship among the colocated processes in some way and thus a measure of system load and behaviour.

Multilocation of process id(s) in multiple time out buckets creates hyperedges amongst the timer bucket vertices and thereby a hypergraph.

References:
-----------
1.Spark MLLib Documentation - https://spark.apache.org/docs/2.3.0/ml-frequent-pattern-mining.html

------------------------------------------------------------------------------------------------------
19 March 2019 - String Search in Large Files
------------------------------------------------------------------------------------------------------
BigData or Large Text Files on clouds often require a functionality to quickly search for a string of patterns or text. There are standard string matching algorithms like Knuth-Morris-Pratt, Boyer-Moore etc., which are standard algorithms implemented in text editors for "find". For large filesystems, wavelet trees are fast alternatives which support rank(c,p) [number of occurrences of character c before position p], select(c,q) [q-th occurrence of character c], access(k)[access k-th position] operations in O(1) time for binary vectors. For substring pattern p of size n, repetitive n invocations of select(p[i],q) returns all substrings of pattern p in the large text file - Example Pseudocode below for first match:
		for k in xrange(n):
			pos=wavelettree.select(p[k],1)
			if prevpos + 1 != pos
				return matchfound==false
			prevpos=pos
		return matchfound==true

------------------------------------------------------------------------------------------------------
26 March 2019 - Example MapReduce in Spark 2.4 Cloud - Bitonic Sequences of Integers
------------------------------------------------------------------------------------------------------
Spark framework parallelizes work by partitioning a bigdata set into Resilient Distributed Datasets
which are map()-ped to Spark cloud nodes by Spark Executor Driver and local computations in cloud node are unified by reduce(). Spark 2.4 + Python Spark code example in code/Spark_MapReduce.py defines two functions map() and reduce(). Spark context is instantiated and an array of integers is parallelized to
Spark resilient distributed dataset partitions and map() is invoked per RDD on Map function - Map() which returns a single element array as a tuple. This is followed by reduce() on Reduce function - Reduce() which takes as args two mapped tuples and merges the integer arrays after > or <= checks to create a subarray which is either strictly ascending or descending. Resultant array is checked for sortedness by a function and mapreduces further if not. Logs for this in Spark_MapReduce.log.26March2019 demonstrate the bitonic sequence n1 + n2 which fluctuate (ascending-descending-ascending):
Reduce: (1, [15], 1, [16], 1, [12], 1, [7], 1, [6], 1, [3], 1, [2], 1, [4], 1, [5])
n1: (1, [15], 1, [16], 1, [12], 1, [7], 1, [6], 1, [3], 1, [2], 1, [4], 1, [5])
n2: (1, [23], 1, [32])
Reduce: (1, [15], 1, [16], 1, [12], 1, [7], 1, [6], 1, [3], 1, [2], 1, [4], 1, [5], 1, [23], 1, [32])
Bitonic Sequence of Integers:
[15, 16, 12, 7, 6, 3, 2, 4, 5, 23, 32]

As can be seen, the reduce produces a huge tuple from which an integer array is extracted after typecheck by Python keyword "type".

------------------------------------------------------------------------------------------------------
16 March 2019 - Set Partitions in SymPy, Survival Index Timeout OS Scheduler 
------------------------------------------------------------------------------------------------------
Spark_FPGrowth_SurvivalIndexTimeout.py example previously described for FPGrowth mining of dictionaries
has been changed to print all possible partitions of set of processes in OS. Partition API in SymPy have been invoked for this. OS Scheduler/Timer is in itself a set partition histogram which maps timeout values to subsets of processes. This demonstrates the integer partition and LSH/set partition-separate chaining isomorphism. Logs in Spark_FPGrowth_SurvivalIndexTimeout.log.16April2019 show the all possible permutations of process set partitions.Rank of a partition is printed which is the difference between size of largest part and number of parts (or) equivalently side of largest square in Durfee Diagram.

References:
-----------
1.Durfee Square - https://en.wikipedia.org/wiki/Durfee_square
2.Rank of a partition and Partitions fitting within a rectangle - Gauss Binomial Coefficient - https://en.wikipedia.org/wiki/Rank_of_a_partition
3.Ferrers Diagram or Young Tableau of a Partition - https://en.wikipedia.org/wiki/Partition_(number_theory)#Ferrers_diagram

---------------------------------------------------------------------------------------------------------
9 July 2019 - Fraud Analytics in Spark 2.4.3
---------------------------------------------------------------------------------------------------------
NeuronRain AstroInfer implements a primitive fraud analytics on a credit card transactions dataset - https://gitlab.com/shrinivaasanka/asfer-github-code/blob/master/python-src/FraudAnalytics.py which analyzes https://www.kaggle.com/mlg-ulb/creditcardfraud csv dataset. Alternatively Spark provides some basic statistics functions for analyzing DataFrames of BigData. An example Spark Python code on Spark 2.4.3 at code/Spark_FraudAnalytics.py does the following :
	(*) Group the transactions by first few columns of cardholder identity and computes average amount withdrawn by aggregating column "Amount"
	(*) Find Frequent Items in the DataFrame
	(*) Describe the columns and compute basic statistics - mean, standard deviation etc.,
...
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+
|summary|                  V1|                  V2|                  V3|                  V4|                  V5|                  V6|                  V7|                  V8|                  V9|            Amount|
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+
|  count|              284807|              284807|              284807|              284807|              284807|              284807|              284807|              284807|              284807|            284807|
|   mean|3.742631994571313...|4.949726613980831...|-7.91956258236933...|2.685625859585728...|-1.52643182031150...|1.813835301123298...|-1.74517780292937...|-2.00384093565998...|-3.14088095662162...| 88.34961925089794|
| stddev|  1.9586958038574895|  1.6513085794769824|   1.516255005177769|  1.4158685749409246|  1.3802467340314435|  1.3322710897575698|   1.237093598182658|  1.1943529026692032|  1.0986320892243098|250.12010924018742|
|    min|-0.00012931370800...|-0.00010296722561...|-0.00010859127517...|-0.00011921826106...|-0.00010366562678...|-0.00010234903761...|-0.00010533581684...|-0.00010065655617...|-0.00010181309940...|                 0|
|    max|7.55406974741191e-05|    9.99769856171626|9.67444968403876e-05|    9.92501936512661|9.99846034625664e-05|    9.91116576052911|    9.97044721041161|    9.90825458583455|9.96754672601408e-05|             999.9|
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+

-----------------------------------------------------------------------------------------------------
21 October 2019 - Advertisement Analytics - Recommender Systems - ALS Collaborative Filtering
-----------------------------------------------------------------------------------------------------
Collaborative Filtering based Recommender Systems works by plotting a matrix of users versus their item choices and predicting preferences of new users or missing preferences based on inferences from the user-items matrix. Advertisement Analytics by computing PageRank of viewer channel switch graph (converging markov random walk) is explained in https://github.com/shrinivaasanka/Grafit/blob/master/course_material/NeuronRain/AdvancedComputerScienceAndMachineLearning/AdvancedComputerScienceAndMachineLearning.txt. Spark MLLib predefines a function for alternating least squares (ALS) collaborative filtering on cloud. Code example in Spark_RecommenderSystems.py which is a modified version of documentation example in https://spark.apache.org/docs/latest/ml-collaborative-filtering.html adapts to channel recommendation systems based on user surveyed matrix of viewer-channel matrix.This is an alternative advertisment analytics.Channel ratings of viewers are read from text file AdvertisementAnalytics_RecommenderSystemsCF.txt having fields - viewer, channelid, rating and timestamp.
ALS factorizes the user-item matrix into User and Item matrix factors and minimizes a quadratic optimization function:
	UserItem = User * Item (Factorization)
	(UserItem - User * Item)^2 (Minimization)
ALS makes either User or Item factor constant alternatingly and converts to a quadratic optimization problem.

------------------------------------------------------------------------------------------------------
23 October 2019, 1 November 2019 - Sequence Mining of Astronomical Datasets by Spark PrefixSpan
------------------------------------------------------------------------------------------------------
NeuronRain Research version of AstroInfer in SourceForge mines astronomical data e.g encoded ephemeris degree locations of celestial bodies, planetary positions on zodiac etc., by native sequential implementation of GSP Sequence Mining algorithm. Spark MLlib provides cloud implementation of an advanced sequence mining algorithm - PrefixSpan which recursively projects a sequence dataset to smaller fine grained datasets and locally mines frequent patterns in smaller databases which are grown (https://ieeexplore.ieee.org/document/1339268 - Mining sequential patterns by pattern-growth: the PrefixSpan approach - Jian Pei,Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada,Jiawei Han,B. Mortazavi-Asl,Jianyong Wang,H. Pinto,Qiming Chen,U. Dayal,Mei-Chun Hsu). An example code for Spark PrefixSpan pattern mining for encoded astronomical datasets (encoding is numeric 1-to-9 for each of the 9 planets spread across 12 zodiac houses delimited by #) is at code/Spark_PrefixSpan.py. It prints frequently occurring celestial juxtapositions of planets in the dataset. Large scale mining of celestial data can be used for variety of scientific applications - solving n-body gravitational differential equations, correlating frequent planetary patterns to terrestrial seismic events and weather forecast. Curiously enough, each encoded string of celestial configuration is a set partition and is an instance of balls-bins problem - set of celestial bodies are bucketed by some permutation amongst 12 houses - number of such celestial configurations are governed by Bell and Stirling numbers which is derived as below (background n-body problem choreography analysis is in NeuronRain FAQ - https://neuronrain-documentation.readthedocs.io/en/latest/):
	Number of ordered partitions of length p of 9 celestial bodies = N(p)
	Summation_p=1-to-9(N(p)) = B9 - 9th Bell Number (= binomial series summation of Stirling Numbers of second kind) = 7087261
	Number of all possible choreographic arrangements of 9 celestial bodies amongst 12 zodiac degree divisions = Summation_p=1-to-m(12CN(p)) which is lowerbounded by B9 >= 7087261 = number of celestial patterns to be correlated to terrestial events. This is a finite number implying repetitive patterns in gravity induced events. 

References:
----------
1.Sage OrderedSetPartition - http://doc.sagemath.org/html/en/reference/combinat/sage/combinat/set_partition_ordered.html

----------------------------------------------------------------------------------------------------------------------------------
18 December 2019, 19 December 2019 - Intrinsic Performance Ratings and Sports Analytics, Streaming Histogram analytics, Partition distance measures, Random graph streams 
----------------------------------------------------------------------------------------------------------------------------------
Non-perceptive intrinsic performance rankings (IPR) have been mentioned as one of the multiple classes of measuring intrinsic fitness/merit as part of People Analytics in https://neuronrain-documentation.readthedocs.io/en/latest/ without voting. IPRs are widely used to rank Chess players (e.g Elo ratings). As an alternative bigdata usecase, stream of cricket match statistics is analyzed for intrinsic merit (Example hawkeye statistics - wagon wheel, trajectories, pitch maps - in https://www.bcci.tv/events/15305/india-v-west-indies-2019/match/15309/1st-odi?tab=overview#hawkeye). NeuronRain Set Partition and Histogram analytics implementations elicit patterns from stream of histogram set partitions by multitude of distance measures - earth mover distance, partition rank, correlation among others:
	(*) Batting and Bowling figures per match for 11 players can be plotted as a histogram of player versus runs/wickets. 
	(*) Streaming Set partition analytics is a special case of Histogram analytics - stream of histograms can have varied total sum of parts while stream of set partitions have oscillating parts but constant total sum of parts.
	(*) Stream of match statistics histograms (which always have constant 11 parts) as time series has the imprint of team performance over the period of time.
	(*) Partition Rank which is defined as maximum part - number of parts is an indicator of skewness in the team performance (maximum runs/wickets scored by a player - 11)
	(*) Durfee square which is the side of largest square inscribed in Ferrer diagram of the match statistics histogram measures if the performance is equitable - larger the square, more players performed nearly equally.
	(*) Time series of Earth mover distance between consecutive match statistics histograms in the stream sheds light on seasonal trend in team performance.
	(*) Patterns in Wagon wheel graphics of shots played per player measures strengths of each player - stream of wagon wheels can be mapped to a histogram of shots and aforementioned metrics apply.  
	(*) Clustering of Bowling trajectories/pitch maps extract stereotypes.

Mining soccer patterns is complex because of random trajectories of the ball which draws a random directed graph on field whose vertices are 2-colored (bichromatic for vertices belonging to two opponent teams). Consecutive match statistics create a stream of 2-colored random graphs and frequent subgraph patterns in this stream of random trajectory graphs show patterns across matches.
